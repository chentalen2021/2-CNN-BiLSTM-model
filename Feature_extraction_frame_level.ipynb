{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# %% Import the libraries for SER; the librosa is the main one for audio analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PyEMD import EMD\n",
    "import opensmile\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(action=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopath = \"/Users/talen/Documents/Datasets/IEMOCAP/Data/Ses01F_impro01_F008.wav\"\n",
    "sig, sr = librosa.load(audiopath, 16000)\n",
    "len(sig) < 2/3 * (3*16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2280, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Prepare the downloaded IEMOCAP dataset for SER\n",
    "# The IEMOCAP dataset was retrieved by requesting to the IEMOCAP development team https://sail.usc.edu/iemocap/release_form.php\n",
    "# Deal with the IEMOCAP metadata to gain the file paths of the improvised speeches in the four desired emotion classes\n",
    "# Read the metadata about the dataset\n",
    "df_descri = pd.read_csv(\"/Users/talen/Documents/Datasets/IEMOCAP/iemocap_metadata.csv\")\n",
    "\n",
    "# Only select the improvised samples to create a description file\n",
    "df_impro = df_descri[df_descri[\"method\"] == \"impro\"]\n",
    "\n",
    "# Replace the default file-path with the local file-path after downloaded in the author's computer\n",
    "new_paths = []\n",
    "\n",
    "# Gain the old paths from \"path\" column of the description file\n",
    "old_paths = df_impro[\"path\"].map(str)\n",
    "for old_path in old_paths:\n",
    "    # Extract the file names\n",
    "    path_list = str(old_path).split(\"/\")\n",
    "    file_name = path_list[-1]\n",
    "\n",
    "    # Concatenate the filename with the local folder path and saved in new_paths variable\n",
    "    new_path = \"/Users/talen/Documents/Datasets/IEMOCAP/Data/\" + file_name\n",
    "    new_paths.append(new_path)\n",
    "\n",
    "# Replace the old paths with the new paths in the description file\n",
    "df_impro.loc[:, [\"path\"]] = new_paths\n",
    "\n",
    "# Select the data about the angry, happy, sad, neutral emotions from the description file\n",
    "df_ang = df_impro[df_impro[\"emotion\"] == \"ang\"]\n",
    "df_hap = df_impro[df_impro[\"emotion\"] == \"hap\"]\n",
    "df_sad = df_impro[df_impro[\"emotion\"] == \"sad\"]\n",
    "df_neu = df_impro[df_impro[\"emotion\"] == \"neu\"]\n",
    "\n",
    "# Concatenate the data of the four emotions\n",
    "df_IEMOCAP = pd.concat([df_ang, df_hap, df_sad, df_neu])\n",
    "df_IEMOCAP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prepare the downloaded EMODB dataset for SER\n",
    "#The dataset was retrieved from and processed according to EMODB data website http://www.emodb.bilderbar.info/download/\n",
    "#10 speakers: 03, 08, 09, 10, 11, 12, 13, 14, 15, 16\n",
    "#Emotion translation: Wut -> angry; Langeweile -> boredom; Ekel -> disgust; Angst -> fear;\n",
    "                    # Freude -> happiness; Trauer -> sadness\n",
    "\n",
    "path_EMODB = \"/Users/talen/Documents/Datasets/EMODB/wav/\"\n",
    "\n",
    "speakers = []\n",
    "emotions = []\n",
    "file_paths = []\n",
    "for folder, _, files in os.walk(path_EMODB):\n",
    "    for file in files:\n",
    "        #Get the speaker ID\n",
    "        speakers.append(file[:2])\n",
    "        #Get the emotion class\n",
    "        if file[5] == \"W\":\n",
    "            emotions.append(\"angry\")\n",
    "        elif file[5] == \"L\":\n",
    "            emotions.append(\"boredom\")\n",
    "        elif file[5] == \"A\":\n",
    "            emotions.append(\"fear\")\n",
    "        elif file[5] == \"F\":\n",
    "            emotions.append(\"happiness\")\n",
    "        elif file[5] == \"T\":\n",
    "            emotions.append(\"sadness\")\n",
    "        elif file[5] == \"E\":\n",
    "            emotions.append(\"disgust\")\n",
    "        else:\n",
    "            emotions.append(\"neutral\")\n",
    "        #Get the file path\n",
    "        file_paths.append(os.path.join(path_EMODB, file))\n",
    "\n",
    "df_EMODB = pd.DataFrame(data={\n",
    "    \"Speaker\":speakers,\n",
    "    \"Emotion\":emotions,\n",
    "    \"File_path\":file_paths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>File_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>boredom</td>\n",
       "      <td>/Users/talen/Documents/Datasets/EMODB/wav/16a0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/talen/Documents/Datasets/EMODB/wav/14a0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/talen/Documents/Datasets/EMODB/wav/10a0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/talen/Documents/Datasets/EMODB/wav/13a0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/talen/Documents/Datasets/EMODB/wav/14a0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speaker  Emotion                                          File_path\n",
       "0      16  boredom  /Users/talen/Documents/Datasets/EMODB/wav/16a0...\n",
       "1      14    angry  /Users/talen/Documents/Datasets/EMODB/wav/14a0...\n",
       "2      10     fear  /Users/talen/Documents/Datasets/EMODB/wav/10a0...\n",
       "3      13  disgust  /Users/talen/Documents/Datasets/EMODB/wav/13a0...\n",
       "4      14    angry  /Users/talen/Documents/Datasets/EMODB/wav/14a0..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_EMODB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for restoring the LLDs, smfcc, their corresponding emotion classes,\n",
    "\n",
    "#For IEMOCAP dataset: emotion classes for 3s-segment-level: ang -> 0, hap -> 1, sad -> 2, neu -> 3\n",
    "Audio_features_IEMOCAP = {\n",
    "    \"1\": {\n",
    "        \"M\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]},\n",
    "        \"F\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]}\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"M\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]},\n",
    "        \"F\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]}\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"M\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]},\n",
    "        \"F\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]}\n",
    "    },\n",
    "    \"4\": {\n",
    "        \"M\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]},\n",
    "        \"F\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]}\n",
    "    },\n",
    "    \"5\": {\n",
    "        \"M\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]},\n",
    "        \"F\":{\"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]}\n",
    "    }\n",
    "}\n",
    "\n",
    "#For EMODB dataset: speakers: 03, 08, 09, 10, 11, 12, 13, 14, 15, 16 \n",
    "Audio_features_EMODB = {\n",
    "    \"03\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"08\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"09\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"13\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"14\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"15\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    },\n",
    "    \"16\": {\n",
    "        \"LLDs\": [], \"Log-Mel-spectram\": [], \"smfcc\": [], \"class\": [], \"LLDs_ori\":[], \"spectrogram_ori\":[]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define the functions for preprocessing and feature extraction\n",
    "\n",
    "# Sampling and quantising the raw audio file into the digital signal\n",
    "def Sampling_and_quantising(file_path):\n",
    "    audiofile = file_path\n",
    "\n",
    "    # Sampling and quantising the audio file into digital signals with the sampling rate of 16kHz\n",
    "    signal, sr = librosa.load(audiofile, sr=16000)\n",
    "\n",
    "    return signal, sr\n",
    "\n",
    "\n",
    "# Extract the LLDs of ComParE_2016 by openSMILE, except for the mfcc-related data\n",
    "def Gain_LLDs(signal, sr):\n",
    "    smile = opensmile.Smile(\n",
    "        feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "        feature_level=opensmile.FeatureLevel.Functionals,\n",
    "    )\n",
    "\n",
    "    LLDs = smile.process_signal(signal, sr)\n",
    "    values_ori = LLDs.copy()\n",
    "    values_ori = values_ori.values[0]\n",
    "    values_ori = values_ori.tolist()\n",
    "\n",
    "    drop_criteria = [\"mfcc\" in x for x in LLDs.columns]\n",
    "\n",
    "    drop_indices = []\n",
    "    for index in range(len(drop_criteria)):\n",
    "        if drop_criteria[index]:\n",
    "            drop_indices.append(LLDs.columns[index])\n",
    "\n",
    "    LLDs.drop(labels=drop_indices, axis=1, inplace=True)\n",
    "\n",
    "    values = LLDs.values[0]\n",
    "\n",
    "    # Restore the LLDs in the Audio_features dictionary\n",
    "    values = values.tolist()\n",
    "\n",
    "    return values, values_ori\n",
    "\n",
    "\n",
    "# Compute the Log-Mel-spectrogram\n",
    "def Gain_Log_Mel_spectrogram(signal, sr, n_fft, n_mels, window):\n",
    "    # Compute the Log-Mel-Spectrogram for each segment\n",
    "    # 1.Compute the spectrogram for each segment short-time FT\n",
    "    stft = librosa.core.stft(y=signal, n_fft=n_fft, window=window)\n",
    "    spectrogram = np.abs(stft)\n",
    "\n",
    "    # 2.Compute the mel-spectrogram by applying filter banks on the spectrogram coefficient\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(sr=sr, S=spectrogram, n_mels=n_mels)\n",
    "\n",
    "    # 3.Compute the logarithm of the mel-spectrogram coefficient\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "#     # Transpose the spectrogram to denote its rows as the time, and columns as the log-mel-spectrogram coefficient\n",
    "#     log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "#     # 4.Get the log-mel-spectrum in the Audio_features dictionary\n",
    "#     log_mel_spectrogram = log_mel_spectrogram.tolist()\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "\n",
    "# Compute the MFCCs\n",
    "def Gain_MFCCs(signal, sr, n_fft, n_mels, n_mfcc, window):\n",
    "    smfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc, dct_type=2, norm=\"ortho\",\n",
    "                                  n_mels=n_mels, n_fft=n_fft, window=window)\n",
    "\n",
    "    # Transpose the SMFCCs so as the row denotes the time\n",
    "    smfccs = smfccs.T\n",
    "\n",
    "    # Store the SMFCCs and their emotion labels under the respective path in the Audio_features dictionary\n",
    "    smfccs = smfccs.tolist()\n",
    "\n",
    "    return smfccs\n",
    "\n",
    "\n",
    "#Generate the spectrogram images\n",
    "def Spectrogram_img(spectro, sr, path):\n",
    "    librosa.display.specshow(spectro, x_axis=\"time\", y_axis=\"mel\", sr=sr)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    plt.savefig(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Calculate frame-level features from raw audio signals\n",
    "def Extract_segment_level_features(current_row):\n",
    "    global h, start_time\n",
    "\n",
    "    # Gain the audio file path, together with its emotion classes, and session path\n",
    "#         #For IEMOCAP dataset\n",
    "#     audiofile = str(current_row[\"path\"])\n",
    "#     emotion = str(current_row[\"emotion\"])\n",
    "#     session = str(current_row[\"session\"])\n",
    "#     gender = str(current_row[\"gender\"])\n",
    "        \n",
    "        #For EMODB dataset\n",
    "    speaker = str(current_row[\"Speaker\"])\n",
    "    emoji = str(current_row[\"Emotion\"])\n",
    "    file_path = str(current_row[\"File_path\"])\n",
    "\n",
    "    # Sampling and quantising the raw audio into the digital signal\n",
    "        #For IEMOCAP dataset -> audiofile; For EMODB dataset -> file_path\n",
    "    signal, sr = Sampling_and_quantising(file_path)\n",
    "\n",
    "    # Check the length of the audio signal to create 3s-segments\n",
    "    audio_length = librosa.get_duration(signal, sr=sr)\n",
    "    No_samples_in_3s = sr * 3\n",
    "\n",
    "    signals_aligned = []\n",
    "    if audio_length < 3:\n",
    "        # Zero-pad the signal if it is less than 3s\n",
    "        signal_padded = librosa.util.fix_length(data=signal, size=No_samples_in_3s)\n",
    "        signals_aligned.append(signal_padded)\n",
    "    else:\n",
    "        # Segment the signal if it is more than 3s\n",
    "        N_3s = int(audio_length / 3)  # How many segments of 3s in current signal\n",
    "        for N_3 in range(N_3s):\n",
    "            # Store the segments of 3s\n",
    "            start_index = N_3 * No_samples_in_3s\n",
    "            end_index = start_index + No_samples_in_3s\n",
    "            signal_segment = signal[start_index:end_index]\n",
    "            signals_aligned.append(signal_segment)\n",
    "\n",
    "            # Zero-pad the rest part\n",
    "        rest_part = signal[N_3s * No_samples_in_3s:]\n",
    "        if len(rest_part) > 2/3 * No_samples_in_3s:\n",
    "            rest_part_padded = librosa.util.fix_length(data=rest_part, size=No_samples_in_3s)\n",
    "            signals_aligned.append(rest_part_padded)\n",
    "        else: pass\n",
    "\n",
    "    # Channel 1: extracting features from original signal\n",
    "    s=0\n",
    "    for signal_aligned in tqdm(signals_aligned):\n",
    "#         # Extract the segment-level LLDs with their functionals from the original signal by openSMILE\n",
    "#         llds, llds_ori = Gain_LLDs(signal=signal_aligned, sr=sr)        \n",
    "        \n",
    "#         #Restore the IEMOCAP data features\n",
    "#         Audio_features_IEMOCAP[session][gender][\"LLDs\"].append(llds)\n",
    "#         Audio_features_IEMOCAP[session][gender][\"LLDs_ori\"].append(llds_ori)\n",
    "\n",
    "#         if emotion == \"ang\":\n",
    "#             Audio_features_IEMOCAP[session][gender][\"class\"].append(0)\n",
    "#         elif emotion == \"hap\":\n",
    "#             Audio_features_IEMOCAP[session][gender][\"class\"].append(1)\n",
    "#         elif emotion == \"sad\":\n",
    "#             Audio_features_IEMOCAP[session][gender][\"class\"].append(2)\n",
    "#         else:\n",
    "#             Audio_features_IEMOCAP[session][gender][\"class\"].append(3)\n",
    "\n",
    "#         #Restore the EMODB data features\n",
    "#         Audio_features_EMODB[speaker][\"LLDs\"].append(llds)\n",
    "#         Audio_features_EMODB[speaker][\"LLDs_ori\"].append(llds_ori)\n",
    "        \n",
    "#         if emoji == \"angry\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(0)\n",
    "#         elif emoji == \"boredom\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(1)\n",
    "#         elif emoji == \"fear\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(2)\n",
    "#         elif emoji == \"happiness\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(3)\n",
    "#         elif emoji == \"sadness\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(4)\n",
    "#         elif emoji == \"disgust\":\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(5)\n",
    "#         else:\n",
    "#             Audio_features_EMODB[speaker][\"class\"].append(6)\n",
    "        \n",
    "        \n",
    "        # Channel 2: extracting features from the signal with trend removed\n",
    "        # Remove signal trend by Zero-crossing detection method\n",
    "        # 1.Use Empirical Mode Decomposition (EMD) method to decompose the signal into IMFs\n",
    "        emd = EMD()\n",
    "        IMFs = emd.emd(signal_aligned, max_imf=9)\n",
    "\n",
    "        # 2. Select the IMFs that satisfy particular criterion\n",
    "        # 2.1 Criterion analysis: ZCR_IMF_i / ZCR_IMF_1 < 0.01  =>  N_ZC_IMF_i / N_ZC_IMF_1 < 0.01, when the IMFs has the same time length\n",
    "        IMFs_selected_index = []\n",
    "\n",
    "        # 2.2 The zero crossing of the first IMF\n",
    "        R_imf_1 = librosa.core.zero_crossings(IMFs[0], pad=False, zero_pos=True)\n",
    "        n_R_imf_1 = sum(R_imf_1)\n",
    "\n",
    "        for i in range(1, len(IMFs)):\n",
    "            R_imf_i = librosa.core.zero_crossings(IMFs[i], pad=False, zero_pos=True)\n",
    "            n_R_imf_i = sum(R_imf_i)\n",
    "\n",
    "            # 2.3 Check the criterion\n",
    "            if n_R_imf_i / n_R_imf_1 < 0.01:\n",
    "                IMFs_selected_index.append(i)\n",
    "\n",
    "            # 3. Derive the signal trend based on the selected IMFs\n",
    "        T = IMFs[0]\n",
    "\n",
    "        for index in range(1, len(IMFs_selected_index)):\n",
    "            T = T + IMFs[index]\n",
    "\n",
    "            # 4. Subtract the signal trend from the original signal\n",
    "        signal_trend_removed = signal_aligned - T\n",
    "\n",
    "\n",
    "        # Extract segment-level spectrograms and SMFCCs from the signal with trend removed\n",
    "        # Calculate the segment-level log-mel-spectrograms by 512 point STFT and 40 mel-filter banks\n",
    "        # Apply the hamming window\n",
    "        spectro = Gain_Log_Mel_spectrogram(signal=signal_trend_removed, sr=sr, n_fft=512, n_mels=40, window=\"ham\")\n",
    "        spectro_ori = Gain_Log_Mel_spectrogram(signal=signal_aligned, sr=sr, n_fft=512, n_mels=40, window=\"ham\")\n",
    "\n",
    "#         # Calculate the 14 smfcc by 512 point STFT and 40 mel-filter banks\n",
    "#         smfcc = Gain_MFCCs(signal=signal_trend_removed, sr=sr, n_fft=512, n_mels=40, n_mfcc=14, window=\"ham\")\n",
    "        \n",
    "        \n",
    "#         #For storing the IEMOCAP data features\n",
    "#         Audio_features_IEMOCAP[session][gender][\"Log-Mel-spectram\"].append(spectro)\n",
    "#         Audio_features_IEMOCAP[session][gender][\"spectrogram_ori\"].append(spectro_ori)\n",
    "#         Audio_features_IEMOCAP[session][gender][\"smfcc\"].append(smfcc)\n",
    "        \n",
    "#         #For storing the EMODB data features\n",
    "#         Audio_features_EMODB[speaker][\"Log-Mel-spectram\"].append(spectro)\n",
    "#         Audio_features_EMODB[speaker][\"spectrogram_ori\"].append(spectro_ori)\n",
    "#         Audio_features_EMODB[speaker][\"smfcc\"].append(smfcc)\n",
    "        \n",
    "        #Store the spectrogram images\n",
    "            #Store the orignial spectrogram\n",
    "        Spectrogram_img(spectro=spectro_ori, sr=sr, \n",
    "                        path=\"/Users/talen/Desktop/Spectrograms_EMODB/ori/\"+str(h)+\".\"+str(s)+\".\"+speaker+\".\"+emoji+\".\"+\"ori\"+\".jpg\")\n",
    "        print(\"Ori spectrogram of utterance{} - segment {} is done!\".format(h, s))\n",
    "            #Store the spectrogram after removing signal trend\n",
    "        Spectrogram_img(spectro=spectro, sr=sr, \n",
    "                        path=\"/Users/talen/Desktop/Spectrograms_EMODB/EMD/\"+str(h)+\".\"+str(s)+\".\"+speaker+\".\"+emoji+\".\"+\"EMD\"+\".jpg\")\n",
    "        print(\"EMD spectrogram of utterance{} - segment {} is done!\".format(h, s))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    used_time = end_time - start_time\n",
    "    rest_time_h = int((used_time / h) * (2280 - h) / 3600)\n",
    "    rest_time_m = int(((used_time / h) * (2280 - h) % 3600) / 60)\n",
    "\n",
    "    print(\"Sample {}/2280 is done!\\nEstimated completion will be {} hour {}mins\".format(h, rest_time_h, rest_time_m))\n",
    "    h += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% Iterate all the speech samples in IEMOCAP dataset\n",
    "h = 1\n",
    "start_time=time.time()\n",
    "\n",
    "for r1 in range(len(df_IEMOCAP)):\n",
    "    row1 = df_IEMOCAP.iloc[r1, :]\n",
    "    Extract_segment_level_features(row1)\n",
    "\n",
    "# Store the Audio_features file locally\n",
    "data_path1 = \"/Users/talen/Desktop/Audio_features_IEMOCAP.json\"\n",
    "\n",
    "with open(data_path1, \"w\") as fp:\n",
    "    json.dump(Audio_features_IEMOCAP, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Iterate all the speech samples in EMODB dataset\n",
    "h = 1\n",
    "start_time=time.time()\n",
    "\n",
    "for r2 in tqdm(range(len(df_EMODB))):\n",
    "    row2 = df_EMODB.iloc[r2, :]\n",
    "    Extract_segment_level_features(row2)\n",
    "\n",
    "# # Store the Audio_features file locally\n",
    "# data_path2 = \"/Users/talen/Desktop/Audio_features_EMODB.json\"\n",
    "\n",
    "# with open(data_path2, \"w\") as fp:\n",
    "#     json.dump(Audio_features_EMODB, fp, indent=4)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
